{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOdx3NRbsYKshxyQSJY5/G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ace96f7b21444eb09bc582a60be0e640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da890bc474a14283abdcc752711f90a0",
              "IPY_MODEL_1aff7dc6155a44d8bf36dd1e0c4c2eab",
              "IPY_MODEL_1bf96d8ccd004dc9a0f9f46d51530b97"
            ],
            "layout": "IPY_MODEL_19890f5bf31d433fbd13362d2b16c294"
          }
        },
        "da890bc474a14283abdcc752711f90a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deca64381567404db2c1a5389b1413cd",
            "placeholder": "​",
            "style": "IPY_MODEL_321cd8cc584f46e585888c9dc54ad61b",
            "value": "yolox_l0.05_quantized.onnx: 100%"
          }
        },
        "1aff7dc6155a44d8bf36dd1e0c4c2eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cf90fa11734bfd857b310f136c5600",
            "max": 54599886,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd55d0c4bef640978179cf0343f8b099",
            "value": 54599886
          }
        },
        "1bf96d8ccd004dc9a0f9f46d51530b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb20b18f27f4b048e81d6bd9f02dd6a",
            "placeholder": "​",
            "style": "IPY_MODEL_51c5fab465784301a3d0e52ce6c09cd1",
            "value": " 54.6M/54.6M [00:00&lt;00:00, 159MB/s]"
          }
        },
        "19890f5bf31d433fbd13362d2b16c294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deca64381567404db2c1a5389b1413cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321cd8cc584f46e585888c9dc54ad61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cf90fa11734bfd857b310f136c5600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd55d0c4bef640978179cf0343f8b099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fb20b18f27f4b048e81d6bd9f02dd6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c5fab465784301a3d0e52ce6c09cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "rK7PQkKzMUNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxLRz5-IhDTI",
        "outputId": "133d4f81-d7be-4308-f198-5cba997d8731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.13 (from langchain)\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.2,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 langchain-0.1.1 langchain-community-0.0.13 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KHX0q_kRK0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(google_api_key = api_key , model = 'gemini-pro', temperature= 0.7)"
      ],
      "metadata": {
        "id": "kWJy2E7yKzyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "TQ0c67fuSwIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585b8ce4-6721-4895-f5f3-4b99d3d620bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.13 (from langchain)\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Downloading langchain_core-0.1.12-py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.13 langchain-core-0.1.12 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "zvj5J9wOKzp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_string = '''Translate {text} in {language} '''\n",
        "my_prompt_template = ChatPromptTemplate.from_template(prompt_string)"
      ],
      "metadata": {
        "id": "4HsvRz8GKzmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_prompt_template.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XkqN9ilKzjL",
        "outputId": "b5aee01d-76d3-403e-c968-9adddc6ed179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], template='Translate {text} in {language} '))]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ =  'Hi How are you ?'\n",
        "language_ =  'Hindi'\n",
        "work  = my_prompt_template.format_messages(text = text_ , language = language_)"
      ],
      "metadata": {
        "id": "MA7GuI2ZKzgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXBtCzZvenqQ",
        "outputId": "638f6ccf-f646-4d55-ca92-eb0e2dedbb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Translate Hi How are you ? in Hindi ')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response =  llm.invoke(work[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDdNSsWiKzXd",
        "outputId": "4f7317df-ae22-471e-c1d9-8448978939e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwc2AxeNekj8",
        "outputId": "0f1d05f1-2f61-4599-8bee-106275500e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='नमस्ते कैसे हो ?')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_message = '''Hey customer , please take care this issue your self as the damage is done by you we are not responsible for this.'''\n",
        "\n",
        "message_style = ''' a polite and professional way'''\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_llm = ''' You are a message modifier and your job is to convert {message} to  {style} and rewrite the message again'''\n",
        "\n",
        "my_prompt =  ChatPromptTemplate.from_template(prompt_llm)"
      ],
      "metadata": {
        "id": "aIPu7xjfg4uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ready_tobe_invoked = my_prompt.format_messages(message = service_message, style = message_style   )\n",
        "\n",
        "ready_tobe_invoked[0].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "N1oVH7mdg4wp",
        "outputId": "64c3d6ba-d4c7-4d11-c058-6f52324593b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You are a message modifier and your job is to convert Hey customer , please take care this issue your self as the damage is done by you we are not responsible for this. to   a polite and professional way and rewrite the message again'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response =  llm.invoke(ready_tobe_invoked[0].content)"
      ],
      "metadata": {
        "id": "5MRDqM_mg4yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "6FPqFE0Ig42K",
        "outputId": "60add2de-a3c0-423d-f008-c765bdab5ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Dear Valued Customer,\\n\\nI understand that you are experiencing an issue with our product, and I apologize for any inconvenience this may have caused. I want to assure you that we value your business and are committed to resolving this matter promptly and professionally.\\n\\nOur customer service team has reviewed your case and determined that the damage to the product was caused by user error. Therefore, we are unable to provide a refund or replacement.\\n\\nHowever, we want to ensure your satisfaction and are willing to offer you a discount on a future purchase. We also have a team of experts who can provide guidance on how to properly use and care for our products.\\n\\nPlease feel free to contact our customer service team if you have any questions or concerns. We are here to help and will do everything we can to make this right.\\n\\nThank you for your understanding.\\n\\nSincerely,\\n\\n[Your Name]\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice Example -3**"
      ],
      "metadata": {
        "id": "CMD3c_zrk9X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "data_format = {\n",
        "  \"gift\": True,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}\n",
        "\n",
        "prompt_llm = '''You need to extract details from {review} and return it in as a json format , a sample format is given as {format}'''\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "before_ready_invoke = ChatPromptTemplate.from_template(prompt_llm)\n",
        "ready_invoke =  before_ready_invoke.format_messages(review = customer_review , format = data_format )\n"
      ],
      "metadata": {
        "id": "mo5dDwVtk7YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(ready_invoke[0].content)"
      ],
      "metadata": {
        "id": "LqTLyTink7aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\".join(response.content.replace('\\n','').split())\n",
        "data = data.replace('\\n', '').replace('```','').strip()\n",
        "json.loads(data).get('price_value')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7N6QCZNis9Mj",
        "outputId": "031ce7b9-00aa-435a-8b4d-d856ded2349b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'slightlymoreexpensive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "raw_data = '{\"gift\":true,\"delivery_days\":2,\"price_value\":\"slightlymoreexpensive\"}'\n",
        "\n",
        "# Remove newline characters and any other unnecessary whitespace\n",
        "cleaned_data = raw_data.replace('\\n', '').strip()\n",
        "\n",
        "try:\n",
        "    data_dict = json.loads(cleaned_data)\n",
        "    print(\"Parsed JSON:\", data_dict)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Error decoding JSON:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn6afIoUvon0",
        "outputId": "68d81283-5d18-4891-8ab0-a2a13c664b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed JSON: {'gift': True, 'delivery_days': 2, 'price_value': 'slightlymoreexpensive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat Conversation using Langchain.chains and Langchain.Memory**"
      ],
      "metadata": {
        "id": "_uESUcJTAfAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converstaion Buffer Memory**"
      ],
      "metadata": {
        "id": "V0qFF_rZLRSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(llm= llm, memory= memory , verbose= True)\n",
        "\n",
        "conversation.invoke('My name is Ashis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGwXOEpRk7kX",
        "outputId": "a67f4de3-1d02-4e9c-f0aa-0065eaebbd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My name is Ashis\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is Ashis',\n",
              " 'history': '',\n",
              " 'response': \"Nice to meet you, Ashis. I'm glad to be chatting with you today.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('What is 2+2 ?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qyOw3us-mDe",
        "outputId": "bc22f7ce-1edc-4482-c7d6-ec56f98a774d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Ashis\n",
            "AI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\n",
            "Human: What is 2+2 ?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is 2+2 ?',\n",
              " 'history': \"Human: My name is Ashis\\nAI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\",\n",
              " 'response': '2+2 is 4.'}"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('I am using langchain chat and memory buffer to store our conversation , i told you my name just before previous conversation, can you tell whats my name?')"
      ],
      "metadata": {
        "id": "Khd1F5rGCCLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('whats my name?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oji9c4cRCedo",
        "outputId": "75d7880f-55e8-4b7f-eaa8-13e56f1cc087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Ashis\n",
            "AI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\n",
            "Human: What is 2+2 ?\n",
            "AI: 2+2 is 4.\n",
            "Human: whats my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'whats my name?',\n",
              " 'history': \"Human: My name is Ashis\\nAI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\\nHuman: What is 2+2 ?\\nAI: 2+2 is 4.\",\n",
              " 'response': 'Your name is Ashis. You told me that earlier in our conversation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lVmEFjvg-mIB",
        "outputId": "c4727f9a-3d32-458a-f3b8-3a82e92d6189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: My name is Ashis\\nAI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\\nHuman: What is 2+2 ?\\nAI: 2+2 is 4.\\nHuman: whats my name?\\nAI: Your name is Ashis. You told me that earlier in our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TPO5JWE-mKT",
        "outputId": "b436cf7f-e430-4478-d911-fa79692cf0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: My name is Ashis\\nAI: Nice to meet you, Ashis. I'm glad to be chatting with you today.\\nHuman: What is 2+2 ?\\nAI: 2+2 is 4.\\nHuman: whats my name?\\nAI: Your name is Ashis. You told me that earlier in our conversation.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation Buffer Window Memory**"
      ],
      "metadata": {
        "id": "qlSCPk_pLJFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = GoogleGenerativeAI(google_api_key= api_key ,model='gemini-pro')\n",
        "memory = ConversationBufferWindowMemory(k = 2)\n",
        "conversation  = ConversationChain(llm = llm, memory= memory)\n",
        "\n"
      ],
      "metadata": {
        "id": "HjHkmCjm-mMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('Hi How are you?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5_BlXCc-mPk",
        "outputId": "5a84d406-bb73-45e7-f238-d2e23f776535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Hi How are you?',\n",
              " 'history': '',\n",
              " 'response': 'Hello! As an AI language model, I don\\'t have personal feelings or emotions, so I don\\'t experience subjective states like \"being.\" However, I\\'m always ready to assist you and provide information to the best of my ability. How may I help you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('I am learning Generatibe AI , Thanks to you')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7AM9e9h-mR6",
        "outputId": "eb419f2e-115e-421f-f174-02cbb26ed39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I am learning Generatibe AI , Thanks to you',\n",
              " 'history': 'Human: Hi How are you?\\nAI: Hello! As an AI language model, I don\\'t have personal feelings or emotions, so I don\\'t experience subjective states like \"being.\" However, I\\'m always ready to assist you and provide information to the best of my ability. How may I help you today?',\n",
              " 'response': \"I'm glad to hear that you're learning about Generative AI! It's an exciting and rapidly developing field with a wide range of applications. If you have any questions or if there's anything I can help you with, feel free to ask. I'll do my best to assist you on your learning journey.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('do you know till when the API of Gemini will be free to use?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTg5GGY0k7mi",
        "outputId": "8fd451ed-3cd9-4af7-c443-c34d2d6ee3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.language_models.llms:Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'do you know till when the API of Gemini will be free to use?',\n",
              " 'history': 'Human: Hi How are you?\\nAI: Hello! As an AI language model, I don\\'t have personal feelings or emotions, so I don\\'t experience subjective states like \"being.\" However, I\\'m always ready to assist you and provide information to the best of my ability. How may I help you today?\\nHuman: I am learning Generatibe AI , Thanks to you\\nAI: I\\'m glad to hear that you\\'re learning about Generative AI! It\\'s an exciting and rapidly developing field with a wide range of applications. If you have any questions or if there\\'s anything I can help you with, feel free to ask. I\\'ll do my best to assist you on your learning journey.',\n",
              " 'response': \"As an AI language model, I don't have real-time information or access to specific company policies. Therefore, I cannot provide you with an accurate answer regarding when the Gemini API will cease to be free to use.\\n\\nTo obtain the most up-to-date and accurate information, I recommend checking the official Gemini website or contacting their customer support team directly. They will be able to provide you with the latest information on their API pricing and policies.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('What is 5+5?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p41cnb0vMMR-",
        "outputId": "ccb114ff-b830-4853-d7b0-0c6f11ec3832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is 5+5?',\n",
              " 'history': \"Human: Hi How are you?\\nAI: I am well, thank you for asking. I am an AI assistant, and I am designed to be helpful and informative. Is there anything I can assist you with today?\\nHuman: I am learning Generatibe AI , Thanks to you\\nAI: You're welcome! I'm glad I can be of assistance. Generative AI is a fascinating field, and I'm happy to help you learn more about it. What specific aspects of Generative AI would you like to discuss?\\nHuman: do you know till when the API of Gemini will be free to use?\\nAI: I do not have access to real-time information and my knowledge cutoff is April 2023. Therefore, I do not have information about the Gemini API's pricing or usage terms. I recommend checking the official Gemini website or contacting their customer support for the most up-to-date information.\",\n",
              " 'response': '5 + 5 = 10'}"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "r4YKDQ2Tk7o_",
        "outputId": "ac494e5e-97f5-414d-8d0a-be40fb42487d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: I am learning Generatibe AI , Thanks to you\\nAI: I'm glad to hear that you're learning about Generative AI! It's an exciting and rapidly developing field with a wide range of applications. If you have any questions or if there's anything I can help you with, feel free to ask. I'll do my best to assist you on your learning journey.\\nHuman: do you know till when the API of Gemini will be free to use?\\nAI: As an AI language model, I don't have real-time information or access to specific company policies. Therefore, I cannot provide you with an accurate answer regarding when the Gemini API will cease to be free to use.\\n\\nTo obtain the most up-to-date and accurate information, I recommend checking the official Gemini website or contacting their customer support team directly. They will be able to provide you with the latest information on their API pricing and policies.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables([])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFFukMJFk7rP",
        "outputId": "784818d4-827f-4bd0-a5bc-81e4fe761c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi How are you?\\nAI: I am well, thank you for asking. I am an AI assistant, and I am designed to be helpful and informative. Is there anything I can assist you with today?\\nHuman: I am learning Generatibe AI , Thanks to you\\nAI: You're welcome! I'm glad I can be of assistance. Generative AI is a fascinating field, and I'm happy to help you learn more about it. What specific aspects of Generative AI would you like to discuss?\\nHuman: do you know till when the API of Gemini will be free to use?\\nAI: I do not have access to real-time information and my knowledge cutoff is April 2023. Therefore, I do not have information about the Gemini API's pricing or usage terms. I recommend checking the official Gemini website or contacting their customer support for the most up-to-date information.\\nHuman: What is 5+5?\\nAI: 5 + 5 = 10\"}"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('I used Langchains buffer window memory to store our conversation with K = 2 that means it shoud not give me all memory convesration but when i am calling memory.load_memory_variables, i am getting all convesration? why?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTmxlF3Dk7ty",
        "outputId": "812802ca-842f-4602-c630-349d04e4885d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I used Langchains buffer window memory to store our conversation with K = 2 that means it shoud not give me all memory convesration but when i am calling memory.load_memory_variables, i am getting all convesration? why?',\n",
              " 'history': \"Human: Hi How are you?\\nAI: I am well, thank you for asking. I am an AI assistant, and I am designed to be helpful and informative. Is there anything I can assist you with today?\\nHuman: I am learning Generatibe AI , Thanks to you\\nAI: You're welcome! I'm glad I can be of assistance. Generative AI is a fascinating field, and I'm happy to help you learn more about it. What specific aspects of Generative AI would you like to discuss?\\nHuman: do you know till when the API of Gemini will be free to use?\\nAI: I do not have access to real-time information and my knowledge cutoff is April 2023. Therefore, I do not have information about the Gemini API's pricing or usage terms. I recommend checking the official Gemini website or contacting their customer support for the most up-to-date information.\\nHuman: What is 5+5?\\nAI: 5 + 5 = 10\",\n",
              " 'response': 'The LangChain buffer window memory with K=2 should only store the last two conversational turns, not the entire conversation history. However, if you are getting all the conversation when calling `memory.load_memory_variables`, it could be due to a few reasons:\\n\\n1. **Incorrect Memory Configuration**: Ensure that you have correctly configured the LangChain buffer window memory with the appropriate value of K. Double-check your code to make sure you are using the correct parameters when initializing the memory.\\n\\n2. **Overwriting Memory**: If you are continuously updating the memory with new conversational turns without properly clearing the old ones, it can lead to the accumulation of the entire conversation history in the memory. Make sure you are clearing the memory appropriately before loading new conversational turns.\\n\\n3. **Bug in the Code**: There might be a bug in the code that is causing the memory to store all the conversation turns instead of just the last two. Check your code carefully for any potential issues, especially in the parts responsible for managing the memory.\\n\\n4. **Incorrect Memory Initialization**: When initializing the LangChain buffer window memory, you might have accidentally set the `max_turns` parameter to a value greater than 2. This would cause the memory to store more than the last two conversational turns.\\n\\n5. **Library Version**: Make sure you are using the latest version of the LangChain library. Sometimes, bugs or issues in older versions can lead to unexpected behavior. Check the documentation or release notes for any known issues or updates related to memory management.\\n\\nTo resolve the issue, you should carefully review your code and ensure that the memory is being configured and managed correctly. If you are still facing problems, consider checking the documentation or reaching out to the LangChain community for assistance.'}"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ConverstaionTokenBufferMemory**"
      ],
      "metadata": {
        "id": "k7Lt8zbtO91u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(google_api_key=api_key, model = 'gemini-pro')\n",
        "memory = ConversationTokenBufferMemory(llm = llm ,max_token_limit= 10)\n",
        "# conversation =  ConversationChain(llm = llm , memory= memory)"
      ],
      "metadata": {
        "id": "9-HopNe3k7wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "id": "AOrL55ohPF1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5doWOho3PF3L",
        "outputId": "864c95f2-defd-497d-dcd3-098e56eb0884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chains**"
      ],
      "metadata": {
        "id": "Gxi6nF-fVa8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm =  ChatGoogleGenerativeAI(max_output_tokens = 50 , google_api_key= api_key , model= 'gemini-pro' , temperature=0.1)"
      ],
      "metadata": {
        "id": "6dfd79ITPF5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "_aJFvR7ePF7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"What is {math}?\")\n",
        "\n",
        "chain = LLMChain(llm = llm , prompt = prompt, verbose= True)"
      ],
      "metadata": {
        "id": "uU5gkjrpPF-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "math = \"5+5\"\n",
        "response = chain.run(math)"
      ],
      "metadata": {
        "id": "j6j2IpZPPGBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "HvgWVc0xPGGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f791e503-73c0-492d-abc6-dd7605b6ed11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_one = ChatPromptTemplate.from_template(\"what is the value of {math}?\")\n",
        "chain_one =  LLMChain(llm = llm, prompt=prompt_one)\n",
        "prompt_two = ChatPromptTemplate.from_template(\"what square root of {number}?\")\n",
        "chain_two = LLMChain(llm=llm , prompt = prompt_two)"
      ],
      "metadata": {
        "id": "d-kfZg9wPGIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "final_chain = SimpleSequentialChain(chains = [chain_one,chain_two],verbose= True)\n"
      ],
      "metadata": {
        "id": "M5VHYsTCPGLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.run('10*10')"
      ],
      "metadata": {
        "id": "Fe7-M-WAPGOD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "39f2c915-9b53-4076-9e0a-50669acfc8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m100\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m10\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "C1XaRK-rKnHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm =  ChatGoogleGenerativeAI(google_api_key= api_key, model = 'gemini-pro', max_output_tokens = 20, temperature=0.9)"
      ],
      "metadata": {
        "id": "VyTGjspkKzno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "p1 = ChatPromptTemplate.from_template('extract numbers from {n1}')\n",
        "c1 =  LLMChain(llm = llm , prompt= p1, output_key= 'o1')\n",
        "p2  = ChatPromptTemplate.from_template('calculate the sum of {o1}')\n",
        "c2  = LLMChain(llm=llm,prompt=p2,output_key='o2')\n",
        "p3 = ChatPromptTemplate.from_template('show the result of {o1} and {o2}' )\n",
        "c3 =  LLMChain(llm=llm, prompt = p3, output_key = 'o3' )\n",
        "\n",
        "final_chain =  SequentialChain(chains = [c1,c2,c3], input_variables=['n1'], output_variables=['o1','o2','o3'],verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "LB7P4j7SPGQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chain.invoke(input = 'The text contains the number as 10 and 20')"
      ],
      "metadata": {
        "id": "184DzGBxPGS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21eeb54f-9c48-46a0-a752-94eff3b53708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "WARNING:langchain_google_genai.chat_models:Gemini produced an empty response.\n",
            "WARNING:langchain_google_genai.chat_models:Unrecognized role: . Treating as a ChatMessage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n1': 'The text contains the number as 10 and 20',\n",
              " 'o1': '- 10\\n- 20',\n",
              " 'o2': '-10 + (-20) = -30',\n",
              " 'o3': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Router Chain:**"
      ],
      "metadata": {
        "id": "ABDcyPD9Wyh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "hYTUTmCGPGVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "XIGNgZ31PGYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "ZZSWyrzNXG7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "lcFiZGn8XOjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "NRkd0K7uXUtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "dCDBw863Xbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "tzUQzq9lXjB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "jXe5piPxXmW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is black body radiation?\")"
      ],
      "metadata": {
        "id": "sfov5Ut0Xont"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "id": "3uIzX9UgXqRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ],
      "metadata": {
        "id": "4ci9lq4GXswm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q & A**"
      ],
      "metadata": {
        "id": "8Lz9tVMtrlGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "wSdCbr-srn-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(model = 'gemini-pro', max_output_tokens = 500, google_api_key= api_key, convert_system_message_to_human=True,temperature = 0.0 )"
      ],
      "metadata": {
        "id": "KnyCNyFVr3A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "with open('kaggle.json') as f:\n",
        "  with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(json.load(f), file)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d ayaz11/used-car-price-prediction\n",
        "!unzip /content/used-car-price-prediction.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Ai5PAnsfYh",
        "outputId": "707dda14-b71f-465b-8cfd-74cfac787208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading used-car-price-prediction.zip to /content\n",
            "\r  0% 0.00/38.8k [00:00<?, ?B/s]\n",
            "\r100% 38.8k/38.8k [00:00<00:00, 59.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/car_web_scraped_dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VHO9ivVLsfge",
        "outputId": "e8df248f-c765-4807-90a4-312fe3d53405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       name  year         miles  \\\n",
              "0                 Kia Forte  2022  41,406 miles   \n",
              "1  Chevrolet Silverado 1500  2021  15,138 miles   \n",
              "2               Toyota RAV4  2022  32,879 miles   \n",
              "3               Honda Civic  2020  37,190 miles   \n",
              "4               Honda Civic  2020  27,496 miles   \n",
              "\n",
              "                               color                       condition    price  \n",
              "0      Gray exterior, Black interior  No accidents reported, 1 Owner  $15,988  \n",
              "1     White exterior, Black interior    1 accident reported, 1 Owner  $38,008  \n",
              "2  Silver exterior, Unknown interior  No accidents reported, 1 Owner  $24,988  \n",
              "3      Blue exterior, Black interior  No accidents reported, 1 Owner  $18,998  \n",
              "4     Black exterior, Black interior  No accidents reported, 1 Owner  $19,498  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee2734f6-e8fa-44ba-83df-8092f68bb6b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>year</th>\n",
              "      <th>miles</th>\n",
              "      <th>color</th>\n",
              "      <th>condition</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kia Forte</td>\n",
              "      <td>2022</td>\n",
              "      <td>41,406 miles</td>\n",
              "      <td>Gray exterior, Black interior</td>\n",
              "      <td>No accidents reported, 1 Owner</td>\n",
              "      <td>$15,988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chevrolet Silverado 1500</td>\n",
              "      <td>2021</td>\n",
              "      <td>15,138 miles</td>\n",
              "      <td>White exterior, Black interior</td>\n",
              "      <td>1 accident reported, 1 Owner</td>\n",
              "      <td>$38,008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Toyota RAV4</td>\n",
              "      <td>2022</td>\n",
              "      <td>32,879 miles</td>\n",
              "      <td>Silver exterior, Unknown interior</td>\n",
              "      <td>No accidents reported, 1 Owner</td>\n",
              "      <td>$24,988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Honda Civic</td>\n",
              "      <td>2020</td>\n",
              "      <td>37,190 miles</td>\n",
              "      <td>Blue exterior, Black interior</td>\n",
              "      <td>No accidents reported, 1 Owner</td>\n",
              "      <td>$18,998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Honda Civic</td>\n",
              "      <td>2020</td>\n",
              "      <td>27,496 miles</td>\n",
              "      <td>Black exterior, Black interior</td>\n",
              "      <td>No accidents reported, 1 Owner</td>\n",
              "      <td>$19,498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee2734f6-e8fa-44ba-83df-8092f68bb6b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee2734f6-e8fa-44ba-83df-8092f68bb6b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee2734f6-e8fa-44ba-83df-8092f68bb6b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d6478174-891d-484e-af2c-49b59ec0a4f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6478174-891d-484e-af2c-49b59ec0a4f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d6478174-891d-484e-af2c-49b59ec0a4f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "MRg5tvoMsfkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain[docarray]\""
      ],
      "metadata": {
        "id": "E-iuiXrL4wvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "emb = GoogleGenerativeAIEmbeddings(google_api_key= api_key,model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "Isc1PmDs2tMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path= '/content/car_web_scraped_dataset.csv')\n",
        "index =  VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch,embedding= emb).from_loaders([loader])"
      ],
      "metadata": {
        "id": "BJIiucrdvdil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "_s2qIAjFBs1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs,\n",
        "    emb\n",
        ")"
      ],
      "metadata": {
        "id": "hiAQeA7m_um-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs = db.similarity_search('List car with one owner')"
      ],
      "metadata": {
        "id": "WIrd3uVmCVgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "rHlhkWsgCiAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_vvMG7MLtun",
        "outputId": "df8c3f3f-2d32-420d-c8be-7006e69867a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7976a66d0490>)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\", # we can use (map_reduce / stuff) instead of refine\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "h3pLa46qsf0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Extract car details with No accidents reported and having 1 Owner\""
      ],
      "metadata": {
        "id": "-c175Kdh-8rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_stuff.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQUMMIZKKt3V",
        "outputId": "a39b9f56-04d1-4225-b26e-b2df811880d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPqwRZqBKyc3",
        "outputId": "710f6277-a345-49c1-a17c-a10d8482d16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Extract car details with No accidents reported and having 1 Owner',\n",
              " 'result': 'Refined Answer:\\n\\nBased on the new context, the BMW 3 Series in question does not meet the criteria of having no accidents reported and 1 owner. Therefore, I cannot extract the requested data.\\n\\nThe provided context contains information about a specific BMW 3 Series with 1 accident reported and 2 owners. Since this vehicle does not meet the criteria of having no accidents reported and 1 owner, I cannot extract the requested data.\\n\\nConclusion:\\n\\nThe new context does not provide any additional information that would allow me to extract the requested data. Therefore, the original answer remains the same.\\n\\nThe BMW 3 Series in question does not meet the criteria of having no accidents reported and 1 owner. Therefore, I cannot extract the requested data.'}"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=emb,\n",
        ").from_loaders([loader])"
      ],
      "metadata": {
        "id": "4SuYsBKoP7nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(query, llm=llm)"
      ],
      "metadata": {
        "id": "ejhRW9aqQJy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "U-k-IsTKQLyo",
        "outputId": "4340c90d-06f2-4f46-e429-f46b87f1ed4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The context does not contain any information about cars with no accidents reported and having 1 owner, so I cannot extract the requested data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "K7FWOHhtp0yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ],
      "metadata": {
        "id": "_E7w3hgMp4JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "xI_HEFX2qo3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = '/content/car_web_scraped_dataset.csv'\n",
        "loader = CSVLoader(file_path=file)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "G_yePhAMp4Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "6PTnBrN6qlXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "emb = GoogleGenerativeAIEmbeddings(google_api_key= api_key,model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "ARKWXJT4qjNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch,embedding=emb).from_loaders([loader])"
      ],
      "metadata": {
        "id": "whXXytuXp4Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model = 'gemini-pro',google_api_key= api_key, temperature=0.1)"
      ],
      "metadata": {
        "id": "XOhI1JYJrFtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "tvKkIfYEq_Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEs7GADbp4Y4",
        "outputId": "d53b8d69-b001-4164-aa44-da8307bbf1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='name: Jeep Wrangler\\nyear: 2018\\nmiles: 44,099 miles\\ncolor: Gray exterior, Black interior\\ncondition: No accidents reported, 2 Owners\\nprice: $30,998', metadata={'source': '/content/car_web_scraped_dataset.csv', 'row': 10})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4c0O4z9p4cs",
        "outputId": "81b66866-34ad-4c35-f2e4-723cdd9c5455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='name: Honda Civic\\nyear: 2020\\nmiles: 11,140 miles\\ncolor: Black exterior, Black interior\\ncondition: No accidents reported, 1 Owner\\nprice: $20,998', metadata={'source': '/content/car_web_scraped_dataset.csv', 'row': 11})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"Does any Jeep Wrangler car has price exactly as $30,998?\",\n",
        "        \"answer\": \"Yes\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Did any Honda Civic car run exactly 11,140?\",\n",
        "        \"answer\": \"Yes\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Zgl04vIjp4gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.qa import QAGenerateChain\n",
        "example_gen_chain = QAGenerateChain.from_llm(llm)"
      ],
      "metadata": {
        "id": "uBbGlp6Q2jwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples = example_gen_chain.apply_and_parse(\n",
        "    [{\"doc\": t} for t in data[:5]]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mkx4T772qTc",
        "outputId": "bf87b409-a369-413e-c379-7199fb7d9c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUL-bB-u28XU",
        "outputId": "ac2cb4ed-2879-4451-cd7f-5236aa5a12e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "   'answer': 'The make is Kia, the model is Forte, and the year is 2022.'}},\n",
              " {'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "   'answer': 'The make, model, and year of the vehicle are Chevrolet Silverado 1500 and 2021.'}},\n",
              " {'qa_pairs': {'query': 'What is the mileage of the 2022 Toyota RAV4?',\n",
              "   'answer': '32,879 miles'}},\n",
              " {'qa_pairs': {'query': 'What is the color of the Honda Civic?',\n",
              "   'answer': 'Blue exterior, Black interior'}},\n",
              " {'qa_pairs': {'query': 'What is the mileage of the Honda Civic?',\n",
              "   'answer': '27,496 miles'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ctvdVd3O3s",
        "outputId": "1e543abf-282f-4b46-b36f-c5eaea4a64c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "  'answer': 'The make is Kia, the model is Forte, and the year is 2022.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgWT4zj_3Yzt",
        "outputId": "44abd72c-ac0f-4fb7-e36e-76f104d4c033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='name: Kia Forte\\nyear: 2022\\nmiles: 41,406 miles\\ncolor: Gray exterior, Black interior\\ncondition: No accidents reported, 1 Owner\\nprice: $15,988', metadata={'source': '/content/car_web_scraped_dataset.csv', 'row': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples += new_examples"
      ],
      "metadata": {
        "id": "gFKQjVre3fNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model = 'gemini-pro',google_api_key= api_key, temperature=0.1,convert_system_message_to_human=True)\n"
      ],
      "metadata": {
        "id": "DQMDvioT4D7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "-dtWyMdV4P9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiIM8yvK5RU1",
        "outputId": "04a87974-9f9b-45c7-bdb6-09d6e3f2f77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='name: Chevrolet Silverado 1500\\nyear: 2021\\nmiles: 15,138 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $38,008', metadata={'source': '/content/car_web_scraped_dataset.csv', 'row': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rok0prH4mcF",
        "outputId": "6c29c320-b300-42e9-b231-a2d96373c866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "  'answer': 'The make, model, and year of the vehicle are Chevrolet Silverado 1500 and 2021.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(new_examples[1][\"qa_pairs\"][\"query\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "aMmlVigr3s_5",
        "outputId": "9b07804e-85bd-4695-85b2-254cc4bcf226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The make, model, and year of the vehicle are Chevrolet Silverado 1500. The year is not specified in the context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True"
      ],
      "metadata": {
        "id": "_eY_43rK5jgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(new_examples[1][\"qa_pairs\"][\"query\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bd9816L05maI",
        "outputId": "18249cb4-7101-4b7f-f65e-93decbb16507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"query\": \"What is the make, model, and year of the vehicle?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"What is the make, model, and year of the vehicle?\",\n",
            "  \"context\": \"name: Chevrolet Silverado 1500\\nyear: 2018\\nmiles: 54,592 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 2 Owners\\nprice: $33,998\\n\\nname: Chevrolet Silverado 1500\\nyear: 2020\\nmiles: 52,538 miles\\ncolor: Silver exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $42,995\\n\\nname: Chevrolet Silverado 1500\\nyear: 2021\\nmiles: 15,138 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $38,008\\n\\nname: Chevrolet Silverado 1500\\nyear: 2021\\nmiles: 15,138 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $38,008\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nname: Chevrolet Silverado 1500\\nyear: 2018\\nmiles: 54,592 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 2 Owners\\nprice: $33,998\\n\\nname: Chevrolet Silverado 1500\\nyear: 2020\\nmiles: 52,538 miles\\ncolor: Silver exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $42,995\\n\\nname: Chevrolet Silverado 1500\\nyear: 2021\\nmiles: 15,138 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $38,008\\n\\nname: Chevrolet Silverado 1500\\nyear: 2021\\nmiles: 15,138 miles\\ncolor: White exterior, Black interior\\ncondition: 1 accident reported, 1 Owner\\nprice: $38,008\\nHuman: What is the make, model, and year of the vehicle?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatGoogleGenerativeAI] [3.95s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Chevrolet Silverado 1500, 2021\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Chevrolet Silverado 1500, 2021\"\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"safety_ratings\": [\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        }\n",
            "      ],\n",
            "      \"block_reason\": \"BLOCK_REASON_UNSPECIFIED\"\n",
            "    }\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [3.95s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Chevrolet Silverado 1500, 2021\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [3.96s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output_text\": \"Chevrolet Silverado 1500, 2021\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [4.18s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"result\": \"Chevrolet Silverado 1500, 2021\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chevrolet Silverado 1500, 2021'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "4s3q8X-e-yij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEQjPUTH_Jcd",
        "outputId": "57b2a02e-102e-40ce-b391-b2f326cd88b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "   'answer': 'The make is Kia, the model is Forte, and the year is 2022.'}},\n",
              " {'qa_pairs': {'query': 'What is the make, model, and year of the vehicle?',\n",
              "   'answer': 'The make, model, and year of the vehicle are Chevrolet Silverado 1500 and 2021.'}},\n",
              " {'qa_pairs': {'query': 'What is the mileage of the 2022 Toyota RAV4?',\n",
              "   'answer': '32,879 miles'}},\n",
              " {'qa_pairs': {'query': 'What is the color of the Honda Civic?',\n",
              "   'answer': 'Blue exterior, Black interior'}},\n",
              " {'qa_pairs': {'query': 'What is the mileage of the Honda Civic?',\n",
              "   'answer': '27,496 miles'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples = [{'query': new_examples['qa_pairs']['query'], 'answer': new_examples['qa_pairs']['answer']} for new_examples in new_examples]\n"
      ],
      "metadata": {
        "id": "6kCu3_iHAC9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iph8IFA3ARRq",
        "outputId": "444c30fa-a945-4515-b55e-b385e420aa39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'query': 'What is the make, model, and year of the vehicle?',\n",
              "  'answer': 'The make is Kia, the model is Forte, and the year is 2022.'},\n",
              " {'query': 'What is the make, model, and year of the vehicle?',\n",
              "  'answer': 'The make, model, and year of the vehicle are Chevrolet Silverado 1500 and 2021.'},\n",
              " {'query': 'What is the mileage of the 2022 Toyota RAV4?',\n",
              "  'answer': '32,879 miles'},\n",
              " {'query': 'What is the color of the Honda Civic?',\n",
              "  'answer': 'Blue exterior, Black interior'},\n",
              " {'query': 'What is the mileage of the Honda Civic?',\n",
              "  'answer': '27,496 miles'}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = qa.batch(new_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi3LjgN5-x0N",
        "outputId": "07bf4073-15b8-4f57-90f7-408cabb3aba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.qa import QAEvalChain"
      ],
      "metadata": {
        "id": "jqNWdccoAdu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_chain = QAEvalChain.from_llm(llm)"
      ],
      "metadata": {
        "id": "xRET8ZGCAe_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graded_outputs = eval_chain.evaluate(new_examples, predictions)"
      ],
      "metadata": {
        "id": "-WzRtr7-Am78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graded_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8QmEWBTBE6c",
        "outputId": "39cacc20-b901-4290-9c8f-03a9ae10f200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'results': 'INCORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'}]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, eg in enumerate(new_examples):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + predictions[i]['query'])\n",
        "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6wq7nxRAxs8",
        "outputId": "05197cbd-e75d-40ef-aaa1-4af8e7324118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "Question: What is the make, model, and year of the vehicle?\n",
            "Real Answer: The make is Kia, the model is Forte, and the year is 2022.\n",
            "Predicted Answer: The make, model, and year of the vehicle are all Chevrolet Silverado 1500.\n",
            "Predicted Grade: INCORRECT\n",
            "\n",
            "Example 1:\n",
            "Question: What is the make, model, and year of the vehicle?\n",
            "Real Answer: The make, model, and year of the vehicle are Chevrolet Silverado 1500 and 2021.\n",
            "Predicted Answer: Chevrolet Silverado 1500, 2021\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 2:\n",
            "Question: What is the mileage of the 2022 Toyota RAV4?\n",
            "Real Answer: 32,879 miles\n",
            "Predicted Answer: The provided context does not specify the mileage of the 2022 Toyota RAV4.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 3:\n",
            "Question: What is the color of the Honda Civic?\n",
            "Real Answer: Blue exterior, Black interior\n",
            "Predicted Answer: The color of the Honda Civic cannot be determined from the provided context.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 4:\n",
            "Question: What is the mileage of the Honda Civic?\n",
            "Real Answer: 27,496 miles\n",
            "Predicted Answer: The provided context does not specify the mileage of the Honda Civic.\n",
            "Predicted Grade: CORRECT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents:**\n",
        "\n",
        "doc: https://python.langchain.com/docs/integrations/tools/\n",
        "\n",
        "https://cobusgreyling.medium.com/agents-large-language-models-5bba2a0c20c7"
      ],
      "metadata": {
        "id": "Yvt4zO1UY1db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "kN4EAFsIY_PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(model = 'gemini-pro',convert_system_message_to_human= True , google_api_key= api_key)"
      ],
      "metadata": {
        "id": "b59Zlpx4Y5U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall langchain\n",
        "!pip install langchain\n",
        "!pip install langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjDGLTMqbYbQ",
        "outputId": "ceed5455-581a-47be-d54f-8e1d7e99849c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.1.1\n",
            "Uninstalling langchain-0.1.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/langchain-server\n",
            "    /usr/local/lib/python3.10/dist-packages/langchain-0.1.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/langchain/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled langchain-0.1.1\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: langchain\n",
            "Successfully installed langchain-0.1.1\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.49-py3-none-any.whl (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.1)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.13)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (0.0.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2,>=0.1->langchain_experimental) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.7->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.7->langchain_experimental) (23.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2,>=0.1->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2,>=0.1->langchain_experimental) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2,>=0.1->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2,>=0.1->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2,>=0.1->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.7->langchain_experimental) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.7->langchain_experimental) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.7->langchain_experimental) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2,>=0.1->langchain_experimental) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2,>=0.1->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2,>=0.1->langchain_experimental) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2,>=0.1->langchain_experimental) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2,>=0.1->langchain_experimental) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2,>=0.1->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2,>=0.1->langchain_experimental) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2,>=0.1->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2,>=0.1->langchain_experimental) (1.0.0)\n",
            "Installing collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjaMhvBRop5C",
        "outputId": "a44cf24d-1a87-427e-c849-7b3519629826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.1.1 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (0.1.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.1) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.1) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain==0.1.1) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.1) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.1) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.1) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n"
      ],
      "metadata": {
        "id": "balp5nvHY5YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lsAtBvFe3eK",
        "outputId": "744e1f9c-42bd-4e65-8113-bcb57e1914b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=e9c0e367e02856854181323de7d1898788763b467b978b6e85e89e1add42bf3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
      ],
      "metadata": {
        "id": "H1dfbMoNY5bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent= initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "CV1bXLSiY5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"What is the 25% of 300?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stTTUnEMY5gx",
        "outputId": "9f2f3c6e-2e22-403f-bae8-c8ef35b2d7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: What is the 25% of 300?\n",
            "Thought: I should calculate the 25% of 300.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"0.25*300\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 75\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the 25% of 300?', 'output': '75'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent('What is 75th percentile of 500?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlPfIxwym_mx",
        "outputId": "45750f76-ae5f-458f-f65e-ab07ae923a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I will use a calculator to find 75% of 500.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": {\n",
            "    \"expression\": \"75/100*500\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 375.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 375\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is 75th percentile of 500?', 'output': '375'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tom M. Mitchell is an American computer scientist \\\n",
        "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
        "what book did he write?\"\n",
        "result = agent(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFx9Y5s7Y5jz",
        "outputId": "69af4cf8-ed2d-4ee9-8160-1d89ef7ca05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: what book did Tom M. Mitchell write?\n",
            "Thought: I need to collect information on Tom M. Mitchell to see if I can answer the question.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Wikipedia\",\n",
            "  \"action_input\": \"Tom M. Mitchell\"\n",
            "}\n",
            "```\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
            "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
            "\n",
            "Page: Ensemble learning\n",
            "Summary: In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
            "Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe information extracted indicated that Tom M. Mitchell wrote the textbook titled \"Machine Learning.\"\n",
            "Final Answer: Machine Learning\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_python_agent(\n",
        "    llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "-ho1sb44q79M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_list = [[\"Harrison\", \"Chase\"],\n",
        "                 [\"Lang\", \"Chain\"],\n",
        "                 [\"Dolly\", \"Too\"],\n",
        "                 [\"Elle\", \"Elem\"],\n",
        "                 [\"Geoff\",\"Fusion\"],\n",
        "                 [\"Trance\",\"Former\"],\n",
        "                 [\"Jen\",\"Ayai\"]\n",
        "                ]"
      ],
      "metadata": {
        "id": "u-DV5EbDq8Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "mKrzGwhmq8LE",
        "outputId": "5f20bf97-f5c5-4b38-d9d7-02e454a4ae83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
            "Action Input: print(sorted(sorted( [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] , key=lambda x: x[1]), key=lambda x: x[0]))\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug=True\n",
        "agent.run(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")\n",
        "langchain.debug=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_29LIloq8PD",
        "outputId": "9c10088c-fc05-42cb-8396-a0da7b8ecdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatGoogleGenerativeAI] [3.49s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Action: Python_REPL\\nAction Input: print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Action: Python_REPL\\nAction Input: print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\"\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"safety_ratings\": [\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        }\n",
            "      ],\n",
            "      \"block_reason\": \"BLOCK_REASON_UNSPECIFIED\"\n",
            "    }\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [3.49s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Action: Python_REPL\\nAction Input: print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] Entering Tool run with input:\n",
            "\u001b[0m\"print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] [3ms] Exiting Tool run with output:\n",
            "\u001b[0m\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"Action: Python_REPL\\nAction Input: print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:Action: Python_REPL\\nAction Input: print(sorted(sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: x[0]), key=lambda x: x[1]))\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatGoogleGenerativeAI] [2.29s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": 9,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 8,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 7,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            },\n",
            "            {\n",
            "              \"category\": 10,\n",
            "              \"probability\": 1,\n",
            "              \"blocked\": false\n",
            "            }\n",
            "          ]\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"safety_ratings\": [\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "          \"probability\": \"NEGLIGIBLE\",\n",
            "          \"blocked\": false\n",
            "        }\n",
            "      ],\n",
            "      \"block_reason\": \"BLOCK_REASON_UNSPECIFIED\"\n",
            "    }\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [2.30s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [5.81s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "EOnjddPVq8SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def time_new(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\"\n",
        "    return str(date.today())"
      ],
      "metadata": {
        "id": "Bvi7yS6iq8Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent= initialize_agent(\n",
        "    [time_new],\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "1QsRZ-vYIXzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "qIxoqJDoL5-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdALb4y9JtDs",
        "outputId": "bafb7b0f-41de-48f9-e5ae-a323921345a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "RKuV_682JsPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "APkSw8lILEex",
        "outputId": "c4ec1be7-d09f-4256-9cfd-18e818619942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gemini: A Family of Highly Capable\\nMultimodal Models\\nGemini Team, Google1\\nThis report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\\nacross image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\\nsizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\\nuse-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\\nadvances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\\nhuman-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\\nart in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\\nGemini models in cross-modal reasoning and language understanding will enable a wide variety of use\\ncases and we discuss our approach toward deploying them responsibly to users.\\n1. Introduction\\nWe present Gemini, a family of highly capable multimodal models developed at Google. We trained\\nGemini jointly across image, audio, video, and text data for the purpose of building a model with both\\nstrong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\\nperformance in each respective domain.\\nGemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\\nperformance and deployability at scale, and Nano for on-device applications. Each size is specifically\\ntailored to address different computational limitations and application requirements. We evaluate\\nthe performance of Gemini models on a comprehensive suite of internal and external benchmarks\\ncovering a wide range of language, coding, reasoning, and multimodal tasks.\\nGemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\\n2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\\net al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\\nOpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\\net al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\\nwork on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\\non neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\\n2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\\nOurmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\\nwe report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\\nbenchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\\ntranslation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\\nMMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\\nsuite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\\nchallenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\\n2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\\n1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\\nreport@google.com\\n©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = agent(\"whats the date today?\")\n",
        "except:\n",
        "    print(\"exception on external access\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bacg-gGBI54A",
        "outputId": "01caf78e-70a5-4c3a-dd46-ee18b9449781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: whats the date today?\n",
            "Thought: I can use time_new to get the date of today\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"time_new\",\n",
            "  \"action_input\": \"\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m2024-01-21\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe current date is 2024-01-21\n",
            "Final Answer: 2024-01-21\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def fun1(text: str) -> str:\n",
        "  '''\n",
        "  This function returns details about Gemini model and you should answer anything related Google gemini\n",
        "  model from this Doc only , anything out side of this , you can say you are not aware.\n",
        "\n",
        "    '''\n",
        "\n",
        "  with open('/content/Gemini_paper.pdf', 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    pdf_text = pdf_reader.pages[0].extract_text()\n",
        "\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "AN6GOj_QMIe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent= initialize_agent(\n",
        "    [fun1],\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "S4yThSRWMIiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = agent(\"what is Gemini Ultra?\")\n",
        "except:\n",
        "    print(\"exception on external access\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUahFTIVMIlz",
        "outputId": "9ddd4b46-c669-484c-a143-75ed5c034d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: what is Gemini Ultra?\n",
            "\n",
            "Thought: I should use the fun1 tool to get information about Gemini Ultra.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"fun1\",\n",
            "  \"action_input\": {\n",
            "    \"text\": \"Gemini Ultra\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGemini: A Family of Highly Capable\n",
            "Multimodal Models\n",
            "Gemini Team, Google1\n",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
            "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
            "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
            "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
            "advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\n",
            "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
            "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
            "Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use\n",
            "cases and we discuss our approach toward deploying them responsibly to users.\n",
            "1. Introduction\n",
            "We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n",
            "Gemini jointly across image, audio, video, and text data for the purpose of building a model with both\n",
            "strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\n",
            "performance in each respective domain.\n",
            "Gemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\n",
            "performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n",
            "tailored to address different computational limitations and application requirements. We evaluate\n",
            "the performance of Gemini models on a comprehensive suite of internal and external benchmarks\n",
            "covering a wide range of language, coding, reasoning, and multimodal tasks.\n",
            "Gemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n",
            "2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\n",
            "et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\n",
            "OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\n",
            "et al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\n",
            "work on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\n",
            "on neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n",
            "2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n",
            "Ourmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\n",
            "we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n",
            "benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n",
            "translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n",
            "MMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\n",
            "suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n",
            "challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n",
            "2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n",
            "report@google.com\n",
            "©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m The Gemini Ultra is a model that is capable of learning across different domains, including image, audio, video, and text.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"fun1\",\n",
            "  \"action_input\": {\n",
            "    \"text\": \"Gemini Ultra Features\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGemini: A Family of Highly Capable\n",
            "Multimodal Models\n",
            "Gemini Team, Google1\n",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
            "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
            "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
            "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
            "advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\n",
            "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
            "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
            "Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use\n",
            "cases and we discuss our approach toward deploying them responsibly to users.\n",
            "1. Introduction\n",
            "We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n",
            "Gemini jointly across image, audio, video, and text data for the purpose of building a model with both\n",
            "strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\n",
            "performance in each respective domain.\n",
            "Gemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\n",
            "performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n",
            "tailored to address different computational limitations and application requirements. We evaluate\n",
            "the performance of Gemini models on a comprehensive suite of internal and external benchmarks\n",
            "covering a wide range of language, coding, reasoning, and multimodal tasks.\n",
            "Gemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n",
            "2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\n",
            "et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\n",
            "OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\n",
            "et al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\n",
            "work on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\n",
            "on neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n",
            "2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n",
            "Ourmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\n",
            "we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n",
            "benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n",
            "translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n",
            "MMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\n",
            "suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n",
            "challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n",
            "2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n",
            "report@google.com\n",
            "©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mGemini Ultra has some notable features.\n",
            "\n",
            "Final Answer: Gemini Ultra is a capable model across different modalities, including image, audio, video, and text. It's known for achieving human-expert performance and advancing the state of the art in 30 out of 32 benchmarks, being the first to do so in the MMLU benchmark.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = agent('''accordning to the document , Please send correspondence to whom? before saying you dont know , please cross verify the mail gemini-1-\n",
        "report@google.com''')\n",
        "except:\n",
        "    print(\"exception on external access\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHO1vpQLMIo7",
        "outputId": "6dadac3a-34f5-41f9-ddba-a3ba48b1cefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: Please send correspondence to whom?\n",
            "Thought: It seems I can answer this question by reading the text.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"fun1\",\n",
            "  \"action_input\": {\n",
            "    \"text\": \"Please send correspondence to whom?\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGemini: A Family of Highly Capable\n",
            "Multimodal Models\n",
            "Gemini Team, Google1\n",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
            "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
            "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
            "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
            "advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\n",
            "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
            "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
            "Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use\n",
            "cases and we discuss our approach toward deploying them responsibly to users.\n",
            "1. Introduction\n",
            "We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n",
            "Gemini jointly across image, audio, video, and text data for the purpose of building a model with both\n",
            "strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\n",
            "performance in each respective domain.\n",
            "Gemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\n",
            "performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n",
            "tailored to address different computational limitations and application requirements. We evaluate\n",
            "the performance of Gemini models on a comprehensive suite of internal and external benchmarks\n",
            "covering a wide range of language, coding, reasoning, and multimodal tasks.\n",
            "Gemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n",
            "2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\n",
            "et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\n",
            "OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\n",
            "et al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\n",
            "work on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\n",
            "on neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n",
            "2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n",
            "Ourmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\n",
            "we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n",
            "benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n",
            "translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n",
            "MMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\n",
            "suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n",
            "challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n",
            "2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n",
            "report@google.com\n",
            "©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThere is an e-mail mentioned with the text: \"gemini-1-report@google.com\", so I suppose it's the answer I'm looking for.\n",
            "Final Answer: Send correspondence to gemini-1-report@google.com\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    result = agent('''accordning to the document , Please send correspondence to whom? ''')\n",
        "except:\n",
        "    print(\"exception on external access\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OpcaSzkMIsj",
        "outputId": "16e3f4b3-03a0-4513-a2f9-0011ec4ca2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The question is asking who correspondence should be sent to. I can find the answer in the document.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"fun1\",\n",
            "  \"action_input\": {\n",
            "    \"text\": \"Please send correspondence to whom?\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGemini: A Family of Highly Capable\n",
            "Multimodal Models\n",
            "Gemini Team, Google1\n",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
            "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
            "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
            "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
            "advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\n",
            "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
            "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
            "Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use\n",
            "cases and we discuss our approach toward deploying them responsibly to users.\n",
            "1. Introduction\n",
            "We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n",
            "Gemini jointly across image, audio, video, and text data for the purpose of building a model with both\n",
            "strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\n",
            "performance in each respective domain.\n",
            "Gemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\n",
            "performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n",
            "tailored to address different computational limitations and application requirements. We evaluate\n",
            "the performance of Gemini models on a comprehensive suite of internal and external benchmarks\n",
            "covering a wide range of language, coding, reasoning, and multimodal tasks.\n",
            "Gemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n",
            "2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\n",
            "et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\n",
            "OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\n",
            "et al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\n",
            "work on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\n",
            "on neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n",
            "2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n",
            "Ourmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\n",
            "we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n",
            "benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n",
            "translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n",
            "MMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\n",
            "suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n",
            "challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n",
            "2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n",
            "report@google.com\n",
            "©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe answer to who correspondence should be sent to can be found in the text.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"fun1\",\n",
            "  \"action_input\": {\n",
            "    \"text\": \"correspondence to whom\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGemini: A Family of Highly Capable\n",
            "Multimodal Models\n",
            "Gemini Team, Google1\n",
            "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
            "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
            "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
            "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
            "advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\n",
            "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
            "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
            "Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use\n",
            "cases and we discuss our approach toward deploying them responsibly to users.\n",
            "1. Introduction\n",
            "We present Gemini, a family of highly capable multimodal models developed at Google. We trained\n",
            "Gemini jointly across image, audio, video, and text data for the purpose of building a model with both\n",
            "strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning\n",
            "performance in each respective domain.\n",
            "Gemini1.0, ourfirstversion, comesinthreesizes: Ultraforhighly-complextasks, Proforenhanced\n",
            "performance and deployability at scale, and Nano for on-device applications. Each size is specifically\n",
            "tailored to address different computational limitations and application requirements. We evaluate\n",
            "the performance of Gemini models on a comprehensive suite of internal and external benchmarks\n",
            "covering a wide range of language, coding, reasoning, and multimodal tasks.\n",
            "Gemini advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al.,\n",
            "2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae\n",
            "et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020;\n",
            "OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang\n",
            "et al., 2023), and video understanding(Alayrac et al., 2022; Chen et al., 2023). It also builds on the\n",
            "work on sequence models (Sutskever et al., 2014), a long history of work in deep learning based\n",
            "on neural networks (LeCun et al., 2015), and machine learning distributed systems (Barham et al.,\n",
            "2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\n",
            "Ourmostcapablemodel,GeminiUltra,achievesnewstate-of-the-artresultsin30of32benchmarks\n",
            "we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding\n",
            "benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech\n",
            "translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on\n",
            "MMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via a\n",
            "suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on\n",
            "challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,\n",
            "2023), that comprises questions about images on multi-discipline tasks requiring college-level subject\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemini-1-\n",
            "report@google.com\n",
            "©2023 Google. All rights reservedarXiv:2312.11805v1  [cs.CL]  19 Dec 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe correspondence should be sent to \"gemini-1-report@google.com\".\n",
            "Final Answer: gemini-1-report@google.com\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzbU7G6FMIv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAq4daSpMIzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDF Extraction from https://arxiv.org/**"
      ],
      "metadata": {
        "id": "E4IvE1vnuZQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQBoMGIE3OJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://arxiv.org/pdf/2312.11805.pdf\"\n",
        "local_filename = \"Gemini_paper.pdf\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    with open(local_filename, 'wb') as pdf_file:\n",
        "        pdf_file.write(response.content)\n",
        "    print(f\"PDF downloaded successfully to {local_filename}\")\n",
        "else:\n",
        "    print(f\"Failed to download PDF. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYC-_za7uY7C",
        "outputId": "a70d9b94-ab76-44c2-8ae6-93a11d73fe76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF downloaded successfully to Gemini_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUGOUnv9uYmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "elxSoudG5Ryu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80df679-d122-4417-f723-b007af1430c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (24.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 0s (2,042 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121701 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[all-docs]\" langchain langchain_community \\\n",
        " chromadb langchain-experimental"
      ],
      "metadata": {
        "id": "xxb1TnLFw7Mv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea98de5b-f57f-4663-a869-164d0035722a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured[all-docs]\n",
            "  Downloading unstructured-0.12.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.0.49-py3-none-any.whl (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Collecting filetype (from unstructured[all-docs])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured[all-docs])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.11.2)\n",
            "Collecting emoji (from unstructured[all-docs])\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured[all-docs])\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured[all-docs])\n",
            "  Downloading python_iso639-2024.1.2-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured[all-docs])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured[all-docs])\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured[all-docs])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.5.0)\n",
            "Collecting unstructured-client (from unstructured[all-docs])\n",
            "  Downloading unstructured_client-0.15.2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.5.2)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.12-py3-none-any.whl (20 kB)\n",
            "Collecting unstructured-inference==0.7.21 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-0.7.21-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.2.1)\n",
            "Collecting pypdf (from unstructured[all-docs])\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msg-parser (from unstructured[all-docs])\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Collecting python-docx (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.2)\n",
            "Collecting onnx (from unstructured[all-docs])\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.5.3)\n",
            "Collecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-8.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.21->unstructured[all-docs]) (0.20.2)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.21->unstructured[all-docs]) (4.8.0.76)\n",
            "Collecting onnxruntime<1.16 (from unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.21->unstructured[all-docs]) (4.35.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Downloading langchain_core-0.1.11-py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.81-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.26.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.3.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[all-docs])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from unstructured[all-docs])\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Collecting coloredlogs (from onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs]) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs]) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[all-docs])\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured[all-docs])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2023.6.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.3.post1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (41.0.7)\n",
            "Collecting Pillow>=3.3.2 (from python-pptx<=0.6.23->unstructured[all-docs])\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.21->unstructured[all-docs]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.21->unstructured[all-docs]) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.21->unstructured[all-docs]) (0.4.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (1.11.4)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (0.16.0+cu121)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.21->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.21)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Collecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (2.1.0)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading pypdfium2-4.26.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.21->unstructured[all-docs]) (3.1.1)\n",
            "Building wheels for collected packages: pypika, langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=848fc317fde929e5fb6eff6d6a336262cecf1ae41dd4439e74f62258507e15c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=3e3d2fcaaddebc18c5875d90b1712865ed471feeb2e04da103991fb6396d2b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=8efb98e28783e10669af5d979e061e2bd10980c312adf2dbf12996e4b4adfc67\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=15e92adf576d7321b8425412ff2d026c88d1c2919100a2a844d0ceea6e4c6065\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built pypika langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: pypika, monotonic, mmh3, filetype, antlr4-python3-runtime, XlsxWriter, websockets, uvloop, typing-extensions, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pypdfium2, pypdf, pypandoc, pulsar-client, portalocker, Pillow, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, onnx, omegaconf, olefile, mypy-extensions, marshmallow, langdetect, jsonpointer, jsonpath-python, importlib-metadata, humanfriendly, httptools, h11, emoji, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, unstructured.pytesseract, typing-inspect, starlette, python-pptx, python-docx, pytesseract, posthog, pikepdf, pdf2image, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, msg-parser, jsonpatch, iopath, coloredlogs, asgiref, pdfminer.six, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, fastapi, dataclasses-json, unstructured-client, timm, pdfplumber, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, unstructured, opentelemetry-instrumentation-fastapi, layoutparser, langchain_community, effdet, langchain, chromadb, unstructured-inference, langchain-experimental\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 XlsxWriter-3.1.9 antlr4-python3-runtime-4.9.3 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 dataclasses-json-0.6.3 deprecated-1.2.14 effdet-0.4.1 emoji-2.9.0 fastapi-0.109.0 filetype-1.2.0 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 iopath-0.1.10 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.1 langchain-core-0.1.11 langchain-experimental-0.0.49 langchain_community-0.0.13 langdetect-1.0.9 langsmith-0.0.81 layoutparser-0.3.4 marshmallow-3.20.2 mmh3-4.1.0 monotonic-1.6 msg-parser-1.2.0 mypy-extensions-1.0.0 olefile-0.47 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.4.0 pdf2image-1.17.0 pdfminer.six-20221105 pdfplumber-0.10.3 pikepdf-8.11.2 portalocker-2.8.2 posthog-3.3.1 pulsar-client-3.4.0 pypandoc-1.12 pypdf-3.17.4 pypdfium2-4.26.0 pypika-0.48.9 pytesseract-0.3.10 python-docx-1.1.0 python-dotenv-1.0.0 python-iso639-2024.1.2 python-magic-0.4.27 python-multipart-0.0.6 python-pptx-0.6.23 rapidfuzz-3.6.1 starlette-0.35.1 timm-0.9.12 typing-extensions-4.9.0 typing-inspect-0.9.0 unstructured-0.12.0 unstructured-client-0.15.2 unstructured-inference-0.7.21 unstructured.pytesseract-0.3.12 uvicorn-0.26.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "image_path = \"./\"\n",
        "pdf_elements = partition_pdf(\n",
        "    \"/content/Gemini_paper.pdf\",\n",
        "    chunking_strategy=\"by_title\",\n",
        "    extract_images_in_pdf=True,\n",
        "    max_characters=3000,\n",
        "    new_after_n_chars=2800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=image_path\n",
        "    )"
      ],
      "metadata": {
        "id": "EvLhzhSm4j1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "ace96f7b21444eb09bc582a60be0e640",
            "da890bc474a14283abdcc752711f90a0",
            "1aff7dc6155a44d8bf36dd1e0c4c2eab",
            "1bf96d8ccd004dc9a0f9f46d51530b97",
            "19890f5bf31d433fbd13362d2b16c294",
            "deca64381567404db2c1a5389b1413cd",
            "321cd8cc584f46e585888c9dc54ad61b",
            "e5cf90fa11734bfd857b310f136c5600",
            "cd55d0c4bef640978179cf0343f8b099",
            "2fb20b18f27f4b048e81d6bd9f02dd6a",
            "51c5fab465784301a3d0e52ce6c09cd1"
          ]
        },
        "outputId": "6d1f68e3-ddef-42aa-b4b5-9d255e8feb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "yolox_l0.05_quantized.onnx:   0%|          | 0.00/54.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ace96f7b21444eb09bc582a60be0e640"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_elements(raw_pdf_elements):\n",
        "    \"\"\"\n",
        "    Categorize extracted elements from a PDF into tables and texts.\n",
        "    raw_pdf_elements: List of unstructured.documents.elements\n",
        "    \"\"\"\n",
        "    tables = []\n",
        "    texts = []\n",
        "    for element in raw_pdf_elements:\n",
        "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "            tables.append(str(element))\n",
        "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "            texts.append(str(element))\n",
        "    return texts, tables\n",
        "\n",
        "texts, tables = categorize_elements(pdf_elements)"
      ],
      "metadata": {
        "id": "haXh-Gt39SfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "SZthySIN_m3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key =  userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "RbOlcKHH_0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(google_api_key= api_key, model = 'gemini-pro')\n",
        "\n",
        "result = llm.invoke(\"Who are you?\")\n"
      ],
      "metadata": {
        "id": "wvGQFiiD-iks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ru6TxYySBseZ",
        "outputId": "96dcf34f-50ab-40a2-dcf7-c9090a9b7cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a large language model, trained by Google.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUKdYNCQEFRu",
        "outputId": "9d741aed-55cb-4e3c-e477-62c17df2b76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lc': 1,\n",
              " 'type': 'constructor',\n",
              " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
              " 'kwargs': {'content': 'I am a large language model, trained by Google.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in"
      ],
      "metadata": {
        "id": "6q7CBet5Ctvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "# Generate summaries of text elements\n",
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "    \"\"\"\n",
        "    Summarize text elements\n",
        "    texts: List of str\n",
        "    tables: List of str\n",
        "    summarize_texts: Bool to summarize texts\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt\n",
        "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "    Give a concise summary of the table or text that is well-optimized for retrieval. Table \\\n",
        "    or text: {element} \"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_text)\n",
        "    empty_response = RunnableLambda(\n",
        "        lambda x: AIMessage(content=\"Error processing document\")\n",
        "    )\n",
        "    # Text summary chain\n",
        "    model = VertexAI(\n",
        "        temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n",
        "    ).with_fallbacks([empty_response])\n",
        "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "    # Initialize empty summaries\n",
        "    text_summaries = []\n",
        "    table_summaries = []\n",
        "\n",
        "    # Apply to text if texts are provided and summarization is requested\n",
        "    if texts and summarize_texts:\n",
        "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "    elif texts:\n",
        "        text_summaries = texts\n",
        "\n",
        "    # Apply to tables if tables are provided\n",
        "    if tables:\n",
        "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "    return text_summaries, table_summaries\n",
        "\n",
        "\n",
        "# Get text, table summaries\n",
        "text_summaries2, table_summaries = generate_text_summaries(\n",
        "    texts[9:], tables, summarize_texts=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYDewiNE9Wqy",
        "outputId": "829f9b56-40cd-4f86-c765-631f7006c81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3cWCYmT91J8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}